{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST WGAN GP\n",
    "This is a improvement of the WGAN, that avoid the gradient clipping by using a regularizing term (Gradient Penalty).\n",
    "\n",
    "#### Changes\n",
    "* Discriminator won't have sigmoid\n",
    "* Generator and Discriminator loss differences\n",
    "* Clipping on Discriminator gradients during training.\n",
    "\n",
    "The discriminator will induce the generator to produce samples similar to the real samples.\n",
    "\n",
    "#### Some notes on interpreting losses\n",
    "On WGAN you might have negative losses, what we want to observe is that both generator/discriminator converges near zero.\n",
    "\n",
    "#### References\n",
    "* [Paper](https://arxiv.org/pdf/1704.00028.pdf)\n",
    "* [Code](https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py)\n",
    "* [Various GAN Implementation on Pytorch](https://github.com/eriklindernoren/PyTorch-GAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Pytorch version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import mnist_data_pytorch as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -rf ./runs\n",
    "writer = SummaryWriter('./runs/train')\n",
    "\n",
    "# Metaparameters\n",
    "num_epochs = 100\n",
    "num_classes = 10\n",
    "latent_size = 64\n",
    "gen_lr = 0.0002\n",
    "disc_lr = 0.0002\n",
    "# Loss weight for gradient penalty\n",
    "lambda_gp = 10\n",
    "b1 = 0.5\n",
    "b2 = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Encoder/Decoder/Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size=100, output_size=784):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_size, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, output_size),\n",
    "            nn.Tanh(),\n",
    "            #nn.Sigmoid() #Sigmoid bit better for MNIST\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.model(z)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size=784):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        real_or_fake = self.model(x)\n",
    "        return real_or_fake\n",
    "\n",
    "# Initialize Networks\n",
    "G = Generator(output_size=784, latent_size=latent_size).to(device)\n",
    "D = Discriminator(input_size=784).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_GP(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    batch_size = real_samples.size(0)\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand(batch_size, 1).to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = (torch.ones(batch_size, 1).fill_(0.0)).to(device)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Optimizers\n",
    "On the paper they used RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_generator = torch.optim.Adam(G.parameters(), lr=gen_lr, betas=(b1, b2))\n",
    "optim_discriminator = torch.optim.Adam(D.parameters(), lr=disc_lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid gradient at index 0 - got [100, 1] but expected shape compatible with [100, 1, 100, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-50b39ee7935d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Simple WGAN Loss for discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;31m# Gradient penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mgradient_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_GP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlambda_gp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-56c99b4b1814>\u001b[0m in \u001b[0;36mcompute_GP\u001b[0;34m(D, real_samples, fake_samples)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0monly_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     )[0]\n\u001b[1;32m     19\u001b[0m     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid gradient at index 0 - got [100, 1] but expected shape compatible with [100, 1, 100, 1]"
     ]
    }
   ],
   "source": [
    "k = 1\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss_G = 0.0\n",
    "    running_loss_D = 0.0\n",
    "    # Iterate over the data\n",
    "    for idx_sample, (real_imgs, _) in enumerate(data.dataloaders['train']):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_imgs = torch.flatten(real_imgs, start_dim=1, end_dim=-1)\n",
    "        batch_size = real_imgs.size()[0]\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        for _ in range(k):\n",
    "            # Generate samples from random noise\n",
    "            z_sample = torch.randn(batch_size, latent_size).to(device)\n",
    "            gen_samples = G(z_sample)\n",
    "            optim_discriminator.zero_grad()\n",
    "            # Simple WGAN Loss for discriminator\n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_GP(D, real_imgs.data, gen_samples.data)\n",
    "            d_loss = -torch.mean(D(real_imgs)) + torch.mean(D(gen_samples.detach())) + lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss.backward()\n",
    "            optim_discriminator.step()\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        optim_generator.zero_grad()\n",
    "        # Sample from distribution Z (z~Z)\n",
    "        z_sample = torch.randn(batch_size, latent_size).to(device)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        gen_samples = G(z_sample)\n",
    "        # Simple WGAN Loss for generator\n",
    "        g_loss = -torch.mean(D(gen_samples))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optim_generator.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss_G += g_loss.item() * batch_size\n",
    "        # Update statistics\n",
    "        running_loss_D += d_loss.item() * batch_size\n",
    "    \n",
    "    # Epoch ends\n",
    "    epoch_loss_generator = running_loss_G / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_discriminator = running_loss_D / len(data.dataloaders['train'].dataset)\n",
    "    \n",
    "    # Send results to tensorboard\n",
    "    writer.add_scalar('train/loss_generator', epoch_loss_generator, epoch)\n",
    "    writer.add_scalar('train/loss_discriminator', epoch_loss_discriminator, epoch)\n",
    "    \n",
    "    # Send images to tensorboard\n",
    "    writer.add_images('train/gen_samples', gen_samples.view(batch_size,1,28,28), epoch)\n",
    "    writer.add_images('train/input_images', real_imgs.view(batch_size,1,28,28), epoch)\n",
    "    \n",
    "    # Send latent to tensorboard\n",
    "    writer.add_histogram('train/latent', z_sample, epoch)\n",
    "    writer.add_histogram('train/X', real_imgs, epoch)\n",
    "    writer.add_histogram('train/G(z)', gen_samples, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples (Unconditioned)\n",
    "Observe that the generated samples are somehow a mix of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_idx=0):\n",
    "    G.eval()\n",
    "    \n",
    "    z = torch.randn(1, latent_size).to(device)\n",
    "    with torch.no_grad(): \n",
    "        generated_sample = G(z)\n",
    "\n",
    "    plt.imshow(generated_sample.view(28,28).cpu().numpy())\n",
    "    plt.title('Generated sample')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(generate_sample, num_idx=widgets.IntSlider(min=0, max=100, step=1, value=0));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
