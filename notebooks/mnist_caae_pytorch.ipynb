{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Conditional Adversarial AutoEncoder\n",
    "Implementation of vanilla (no CNN) Conditional Adversarial AutoEncoder. This architecture forces the latent space of an autoencoder to follow a particular distribution, in a way more efficient than Variational AutoEncoders.\n",
    "\n",
    "In other words it makes a simple autoencoder become a generative model.\n",
    "\n",
    "The main difference is that we will inject (concatenate) our label information (one-hot) into the latent space on the decoder, no change in loss or anything else.\n",
    "\n",
    "The main use of this architecture is to augment datasets.\n",
    "\n",
    "#### Losses\n",
    "* Reconstruction Loss\n",
    "* Discriminator Loss\n",
    "* Generator (encoder) Loss\n",
    "\n",
    "The adversarial training criterion forces the autoencoder latent follow any particular distribution.\n",
    "\n",
    "#### References\n",
    "* [Paper](https://arxiv.org/pdf/1511.05644.pdf)\n",
    "* https://github.com/neale/Adversarial-Autoencoder\n",
    "* https://github.com/bfarzin/pytorch_aae\n",
    "* https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Pytorch version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import mnist_data_pytorch as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -rf ./runs\n",
    "writer = SummaryWriter('./runs/train')\n",
    "\n",
    "# Metaparameters\n",
    "num_epochs = 300\n",
    "num_classes = 10\n",
    "latent_size = 100\n",
    "gen_lr = 0.0001\n",
    "reg_lr = 0.00005\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to convert labels to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: one-hot tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "9: one-hot tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "def one_hot(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    Convert labels to one_hot_encoding\n",
    "    \"\"\"\n",
    "    # Convert to One Hot Encoding\n",
    "    y = torch.eye(num_classes)\n",
    "    return y[labels]\n",
    "\n",
    "print('1: one-hot', one_hot(1))\n",
    "print('9: one-hot', one_hot(9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Encoder/Decoder/Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, 1000)\n",
    "        #self.lin1_bn = nn.BatchNorm1d(1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.latent = nn.Linear(1000, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        z = self.latent(x)\n",
    "        return z\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim, num_classes=10):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim + num_classes, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.lin3 = nn.Linear(1000, X_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return torch.sigmoid(x)\n",
    "        #return x\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):  \n",
    "    def __init__(self, z_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.lin3 = nn.Linear(1000, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        return torch.sigmoid(self.lin3(x)) \n",
    "\n",
    "\n",
    "# Initialize Networks\n",
    "encoder = Encoder(784, latent_size).to(device)\n",
    "decoder = Decoder(784, latent_size).to(device)\n",
    "discriminator = Discriminator(latent_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_encoder = torch.optim.Adam(encoder.parameters(), lr=gen_lr)\n",
    "optim_decoder = torch.optim.Adam(decoder.parameters(), lr=gen_lr)\n",
    "#regularizing optimizers\n",
    "optim_encoder_generator = torch.optim.Adam(encoder.parameters(), lr=reg_lr)\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 159/300 [19:39<17:12,  7.32s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss_discriminator = 0.0\n",
    "    running_loss_generator = 0.0\n",
    "    running_loss_reconstruction = 0.0\n",
    "    # Iterate over the data\n",
    "    for idx_sample, (inputs, labels) in enumerate(data.dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = torch.flatten(inputs, start_dim=1, end_dim=-1)\n",
    "        labels = labels.to(device)\n",
    "        # Convert y to one-hot and send to GPU\n",
    "        y = one_hot(labels)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optim_encoder.zero_grad()\n",
    "        optim_decoder.zero_grad()\n",
    "        optim_discriminator.zero_grad()\n",
    "        \n",
    "        # Concatenate label (one-hot) with latent\n",
    "        z_sample = encoder(inputs)\n",
    "        z_sample = torch.cat((y, z_sample), 1)\n",
    "        \n",
    "        # Reconstruct X\n",
    "        inputs_reconstruct = decoder(z_sample)\n",
    "        #reconstruct_loss = F.binary_cross_entropy(inputs_reconstruct + EPS, inputs + EPS)\n",
    "        reconstruct_loss = F.mse_loss(inputs_reconstruct , inputs)\n",
    "        \n",
    "        # Backprop from reconstruction loss\n",
    "        reconstruct_loss.backward()\n",
    "        # Optimizer Encoder/Decoder\n",
    "        optim_encoder.step()\n",
    "        optim_decoder.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss_reconstruction += reconstruct_loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Discriminator (Force latent space to be the same distribution given)\n",
    "        ## true prior is random normal (randn)\n",
    "        ## this is constraining the Z-projection to be normal!\n",
    "        encoder.eval()\n",
    "        batch_size = inputs.size()[0]\n",
    "        z_real_distribution = (torch.randn(batch_size, latent_size) * 1.).to(device)\n",
    "        D_real_gauss = discriminator(z_real_distribution)\n",
    "\n",
    "        # Fake images come from encoder(generator)\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + EPS) + torch.log(1 - D_fake_gauss + EPS))\n",
    "\n",
    "        D_loss.backward()\n",
    "        optim_discriminator.step()\n",
    "        # Update statistics\n",
    "        running_loss_discriminator += D_loss.item() * inputs.size(0)\n",
    "\n",
    "        # Generator\n",
    "        encoder.train()\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + EPS))\n",
    "\n",
    "        G_loss.backward()\n",
    "        optim_encoder_generator.step()   \n",
    "        # Update statistics\n",
    "        running_loss_generator += G_loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Epoch ends\n",
    "    epoch_loss_generator = running_loss_generator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_discriminator = running_loss_discriminator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_reconstruction = running_loss_reconstruction / len(data.dataloaders['train'].dataset)\n",
    "    \n",
    "    # Send results to tensorboard\n",
    "    writer.add_scalar('train/loss_generator', epoch_loss_generator, epoch)\n",
    "    writer.add_scalar('train/loss_discriminator', epoch_loss_discriminator, epoch)\n",
    "    writer.add_scalar('train/reconstruction', epoch_loss_reconstruction, epoch)\n",
    "    \n",
    "    # Send images to tensorboard\n",
    "    writer.add_images('train/decoder_images', inputs_reconstruct.view(inputs.size(0),1,28,28), epoch)\n",
    "    writer.add_images('train/input_images', inputs.view(inputs.size(0),1,28,28), epoch)\n",
    "    \n",
    "    # Send latent to tensorboard\n",
    "    writer.add_histogram('train/latent', z_sample, epoch)\n",
    "    writer.add_histogram('train/labels', labels, epoch)\n",
    "    writer.add_histogram('train/distribution', z_real_distribution, epoch)\n",
    "    writer.add_histogram('train/input_images_h', inputs, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples (Unconditioned)\n",
    "Observe that the generated samples are somehow a mix of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_idx=0, category=0):\n",
    "    decoder.eval()\n",
    "    # Observe that we had to make the labels 10x bigger\n",
    "    one_hot_category = one_hot([category]).to(device) * 10\n",
    "    z_real_gauss = (torch.randn(1, latent_size) * 1.).to(device)\n",
    "    z_real_gauss = torch.cat((one_hot_category, z_real_gauss), 1)\n",
    "    with torch.no_grad(): \n",
    "        generated_sample = decoder(z_real_gauss)\n",
    "\n",
    "    plt.imshow(generated_sample.view(28,28).cpu().numpy())\n",
    "    plt.title('Generated sample')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dd03a955074da6bb871ba4aec10e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='num_idx'), Dropdown(description='category', options=(0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(generate_sample, num_idx=widgets.IntSlider(min=0, max=100, step=1, value=0), category = [x for x in range(10)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAASzElEQVR4nO3dfZRcdX3H8fdnN5vEPElCSIgQESiIiCXCFqi0FosiRM8BFa30iFHRYCuncoqtHDytsT1W6hPSY32IlRqf8AkRqtRCY5VSC2WhMYkEBDGQkJAESZrwlOzDt3/cGxyWnd/szuPu/j6vc+bszP3dO/Odu/OZe+f+7oMiAjOb/Lo6XYCZtYfDbpYJh90sEw67WSYcdrNMOOxmmXDYbcwkvU3SLZ2uI0XSRkmv7HQd44nD3iSS3izpNkmPS9pe3v9TSep0bcNJ+rGkd3a6Dmsvh70JJF0CXAl8DDgYWAi8GzgVmNrmWqa08/VsAokI3xq4Ac8FHgfeUGO8acDHgQeBbcDngOeUbacBm4FLgO3AVuDtY5z2/cDDwFeAucD3gR3AzvL+oeX4HwYGgaeAx4BPl8OPAW4CHgXuAd5U8foHAtcDu4H/Af4WuKXK+5wOfBX4NbALuB1YWLa9HdgA7AHuBy6smG7/+/jLinlwDrAU+EVZ12UV468AvgN8s3y+O4HjK9o3Aq8s73cBlwK/LOv6FjCv05+dtn9WO13ARL8BZwIDwJQa432qDMw8YDbwL8BHyrbTyuf4G6Cn/IA/Acwdw7R/X34pPKcM5xuAGeX43wa+V1HLj4F3VjyeCWwqwzgFOAF4BHhx2f6NMiAzgeOAhxJhv7CsbwbQDZwIzCnbXgMcCQj4g/I9njDsffx1OQ/eRfFl9fXyPbyY4gvqiHL8FUA/cG45/vuAXwE9ZXtl2C8GbgUOLefR54GrO/3ZaftntdMFTPQb8Bbg4WHDfkqxVHsSeHn54X4cOLJinN8FflXeP60cd0pF+3bglFFOuw+YnqhxCbCz4vHwsP8R8J/Dpvk88MEysP3AMRVtf5cI+zvK9//bo5h33wPeO2wedJePZwMBnFwx/h3AOeX9FcCtFW1dFGsDv18+rgz7BuD0inEXle8p+QU92W7+fde4XwPzJU2JiAGAiHgZgKTNFB/CgyiWdHdUbK8TRZCefp7905eeAGaNctodEfHU043SDOAKirWOueXg2ZK6I2JwhPdwGHCypF0Vw6ZQ/CQ4qLy/qaLtgZFnBZTTLAa+IekAilX6D0REv6SzKL5AjqaYLzOAdcPmwf76niz/bqtof5Jinuz3dE0RMVTO7+dVeX/XShqqGDZIsW3locR7mVS8ga5x/w3sBc5OjPMIxQf1xRFxQHl7bkTMSkwzlmmHH7p4CfBCiqXiHIq1Cyi+JEYafxPwk4rnPyAiZkXEn1CsSg9QBHi/51crNiL6I+JDEXEs8DLgtcBbJU0DrqHY9rAwIg4AbqioqR5P1ySpi2I1fcsI420Czhr2/qZHRDZBB4e9YRGxC/gQ8BlJ50qaJalL0hKK37hExBDwBeAKSQsAJB0i6dWjeP56pp1N8QWxS9I8iqVppW3AERWPvw8cLel8ST3l7Xckvahc0n4XWCFphqRjgWXVXljSKyS9RFI3xQa9foql6FSK38s7gIFyKX9Grfdfw4mSXl/2QFxM8aV76wjjfQ74sKTDyhoPkpT6cp6UHPYmiIiPAn/Ob7Ykb6P4zft+it+vlPfvA26VtBv4d4ql72iMddpPUWyoe4Tiw//DYe1XAudK2inpHyJiD0Xw3kyxZHyY32zwA7iIYvX5YeBLwD8nXvtgiq3kuyl+K/8E+Gr5Gn9GsaFvJ/DHFBsdG3EdxfaGncD5wOsjon+E8a4sX+tGSXso5snJDb72hKNyg4XZhCJpBfBbEfGWTtcyUXjJbpYJh90sE16NN8uEl+xmmWjrTjVTNS2mF71RZtYCT/E4+2LviPsuNBR2SWdSdGt0A/8UEZenxp/OTE7W6Y28pJkl3Barq7bVvRpf7jTxj8BZwLHAeeUOF2Y2DjXym/0k4L6IuD8i9lEcGZXdXklmE0UjYT+EZx4csbkc9gySlkvqk9TXz94GXs7MGtFI2EfaCPCsfryIWBkRvRHR2/P03pdm1m6NhH0zzzwSqtoRR2Y2DjQS9tuBoyQdLmkqxUEUjR7YYGYtUnfXW0QMSLoI+DeKrrerIuLnTavMzJqqoX72iLiB4gQEZjbOeXdZs0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulomGLtksaSOwBxgEBiKitxlFmVnzNRT20isi4pEmPI+ZtZBX480y0WjYA7hR0h2Slo80gqTlkvok9fWzt8GXM7N6Nboaf2pEbJG0ALhJ0t0RcXPlCBGxElgJMEfzosHXM7M6NbRkj4gt5d/twLXASc0oysyar+6wS5opafb++8AZwPpmFWZmzdXIavxC4FpJ+5/n6xHxw6ZUZWZNV3fYI+J+4Pgm1mJmLeSuN7NMOOxmmXDYzTLhsJtlwmE3y0QzDoSZELqmT0+3HzQ//QQDA1WbBnekjwOKxLRWXddxx6RH6FayeehnG5pYzcTnJbtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulonJ08+udJ9rRI2T5HSnv/dixqzqk06fln7uoaH0cz9V43RdTz6Vnj7Rj68Zz0k/9YmHJ9u3vGNfsv2co9cm2/9i/n9VbZvfPTM5LaxJtj448Fiy/d3HLa3aNrh7d43Xnny8ZDfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMjF5+tlr9KPHvnR/cdToy+5ffGDVtofe25+c9nMnfDXZfvzUJ5PtMzQ12d5F9X0MutXp7/Nafen1m9+Vni8cenD1trvcz25mk5TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTIxefrZG6Sp6T7bh0+pflz4a45IH3c9lcFk+1CNfQT+L9L7AKSOlj+wK308e+f74ev3QI3z8Q/OqnGegczU/E9LukrSdknrK4bNk3STpHvLv3NbW6aZNWo0X+tfAs4cNuxSYHVEHAWsLh+b2ThWM+wRcTPw6LDBZwOryvurgHOaXJeZNVm9P9gWRsRWgPLvgmojSlouqU9SXz81zrVmZi3T8q0zEbEyInojorcHbzAx65R6w75N0iKA8u/25pVkZq1Qb9ivB5aV95cB1zWnHDNrlZr97JKuBk4D5kvaDHwQuBz4lqQLgAeBN7ayyGbQlJ5kezz2eLJ90S3V23+0+5TktHddV3WTBgAD22qsGNU6530DumbPTrbvefWxyfbnXXxfsv0zh11fta3WeeMHI32+/Xv75yfbp2zdWbUt3UM/OdUMe0ScV6Xp9CbXYmYtNHF3nzKzMXHYzTLhsJtlwmE3y4TDbpYJ1byUcRPN0bw4WQ1sxK9xWeaUrmnpvfeG9qVPB52eOH0I66RW43/yh2urX1b5ffPuSU47RPqz+dq7z06286qtiSefnP+z22I1u+PREf8pXrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpmYWKeSbmCfgKGn0qdjttZ4z9x1Vdu6NT09cY1DXHd8e3Gy/aChzennz4yX7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJiZWP7uNO1MWpk+TPaurRl96Qq3j2RfcvjvZ3r4zNUwMXrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplwP7s15Jgf7OjYaw9NS39867/KwORUc8ku6SpJ2yWtrxi2QtJDktaUt6WtLdPMGjWa1fgvAWeOMPyKiFhS3m5obllm1mw1wx4RNwOPtqEWM2uhRjbQXSRpbbmaP7faSJKWS+qT1NfP3gZezswaUW/YPwscCSwBtgKfqDZiRKyMiN6I6O0hfXFFM2udusIeEdsiYjAihoAvACc1tywza7a6wi5pUcXD1wHrq41rZuNDzX52SVcDpwHzJW0GPgicJmkJxSHDG4ELW1ijdVD33KqbYwD40MJ/rfEM9R/P/thQehtPz0Pp7cYDdb/y5FQz7BFx3giDv9iCWsyshby7rFkmHHazTDjsZplw2M0y4bCbZcKHuFrS3Z86PNk+TT3J9sHEZZcHGExOe+KPLkq2H/XAncl2eyYv2c0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTLifPXdKn3C5Z3P67EJ7oz/ZvmWwel/6xx4+Iznt0ResTbb7ksxj4yW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJ97PnLtK91dO3p/vh7+rvTrb/1f3nVm2b8tbqx7oDxMCWZLuNjZfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmRnPJ5sXAl4GDgSFgZURcKWke8E3gBRSXbX5TROxsXanWCXM2pc/t/q6fvTU9/ao5VdtmbrujrpqsPqNZsg8Al0TEi4BTgPdIOha4FFgdEUcBq8vHZjZO1Qx7RGyNiDvL+3uADcAhwNnAqnK0VcA5rSrSzBo3pt/skl4AvBS4DVgYEVuh+EIAFjS7ODNrnlGHXdIs4Brg4ojYPYbplkvqk9TXz956ajSzJhhV2CX1UAT9axHx3XLwNkmLyvZFwPaRpo2IlRHRGxG9PaRPXmhmrVMz7JIEfBHYEBGfrGi6HlhW3l8GXNf88sysWUZziOupwPnAOklrymGXAZcD35J0AfAg8MbWlGidNPueXcn2ns/MTrfveqJqWwz5ZNDtVDPsEXELUO2g5tObW46ZtYr3oDPLhMNulgmH3SwTDrtZJhx2s0w47GaZ8KmkLSl60qeK3nVET41nqN5+8N0zk1MO7h71Xtk2Cl6ym2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcD977pS+JHPt6dPNO19S/VTUcza9MDnttB/cXk9FVoWX7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJtzPnjlNSR+PHlPSy4O59+xLtu88vvrx8Nt606/9/B8km22MvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTJRs59d0mLgy8DBwBCwMiKulLQCeBewoxz1soi4oVWFWmsMnHpcsn3aL7enn2DdjmTzUXtfVLVt6cobk9N+Z/2rk+0zr7kt2W7PNJqdagaASyLiTkmzgTsk3VS2XRERH29deWbWLDXDHhFbga3l/T2SNgCHtLowM2uuMf1ml/QC4KXA/vWniyStlXSVpLlVplkuqU9SXz97GyrWzOo36rBLmgVcA1wcEbuBzwJHAksolvyfGGm6iFgZEb0R0dvDtCaUbGb1GFXYJfVQBP1rEfFdgIjYFhGDETEEfAE4qXVlmlmjaoZdkoAvAhsi4pMVwxdVjPY6YH3zyzOzZhnN1vhTgfOBdZLWlMMuA86TtAQIYCNwYUsqtJaaumFzeoSu9PIg+geS7d3/+4uqbas+uzQ57fNu3ZhsT7+yDTearfG3MPLZwd2nbjaBeA86s0w47GaZcNjNMuGwm2XCYTfLhMNulgmfSjpzg9tqHMLaoKEnnqjatuDTP01O63705vKS3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhCKifS8m7QAeqBg0H3ikbQWMzXitbbzWBa6tXs2s7bCIOGikhraG/VkvLvVFRG/HCkgYr7WN17rAtdWrXbV5Nd4sEw67WSY6HfaVHX79lPFa23itC1xbvdpSW0d/s5tZ+3R6yW5mbeKwm2WiI2GXdKakeyTdJ+nSTtRQjaSNktZJWiOpr8O1XCVpu6T1FcPmSbpJ0r3l3xGvsdeh2lZIeqicd2skpU8M37raFkv6D0kbJP1c0nvL4R2dd4m62jLf2v6bXVI38AvgVcBm4HbgvIi4q62FVCFpI9AbER3fAUPSy4HHgC9HxHHlsI8Cj0bE5eUX5dyIeP84qW0F8FinL+NdXq1oUeVlxoFzgLfRwXmXqOtNtGG+dWLJfhJwX0TcHxH7gG8AZ3egjnEvIm4GHh02+GxgVXl/FcWHpe2q1DYuRMTWiLizvL8H2H+Z8Y7Ou0RdbdGJsB8CbKp4vJnxdb33AG6UdIek5Z0uZgQLI2IrFB8eYEGH6xmu5mW822nYZcbHzbyr5/LnjepE2Ee6lNR46v87NSJOAM4C3lOurtrojOoy3u0ywmXGx4V6L3/eqE6EfTOwuOLxocCWDtQxoojYUv7dDlzL+LsU9bb9V9At/7b2jJFjMJ4u4z3SZcYZB/Ouk5c/70TYbweOknS4pKnAm4HrO1DHs0iaWW44QdJM4AzG36WorweWlfeXAdd1sJZnGC+X8a52mXE6PO86fvnziGj7DVhKsUX+l8AHOlFDlbqOAH5W3n7e6dqAqylW6/op1oguAA4EVgP3ln/njaPavgKsA9ZSBGtRh2r7PYqfhmuBNeVtaafnXaKutsw37y5rlgnvQWeWCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZeL/AXvMxQ/waYdtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sample(0,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
