{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN\n",
    "CycleGAN Implementation.\n",
    "\n",
    "#### References\n",
    "* [Paper](https://arxiv.org/pdf/1611.07004.pdf)\n",
    "* [Confluece page](https://machinereinforcedbook.atlassian.net/wiki/spaces/ML/pages/22052990/Pix2Pix)\n",
    "* [Medium Article](https://medium.com/datadriveninvestor/style-transferring-of-image-using-cyclegan-3cc7aff4fe61)\n",
    "* [Medium Article](https://blog.paperspace.com/unpaired-image-to-image-translation-with-cyclegan/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Number of GPUs Available: 8\n",
      "Pytorch version: 1.2.0\n",
      "Patch size: (1, 16, 16)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset_utils.img_folder_dataset_cycle import ImageDataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "num_gpu = torch.cuda.device_count()\n",
    "#num_gpu = 8\n",
    "print('Number of GPUs Available:', num_gpu)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -rf ./runs\n",
    "writer = SummaryWriter('./runs/train')\n",
    "\n",
    "# Metaparameters\n",
    "learning_rate = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "img_height = img_width = 256\n",
    "n_residual_blocks = 9\n",
    "dataset_name = 'maps'\n",
    "batch_size = 5\n",
    "num_epochs = 200\n",
    "lambda_cycle = 10.0\n",
    "lambda_identity = 5.0\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n",
    "print('Patch size:', patch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoaders and Replay Buffer\n",
    "\n",
    "The replay Buffer will bring back previously generated samples, this is useful on the Forward/Backward cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        assert max_size > 0, \"Empty buffer or trying to create a black hole. Be careful.\"\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "\n",
    "    def push_and_pop(self, data):\n",
    "        to_return = []\n",
    "        for element in data.data:\n",
    "            element = torch.unsqueeze(element, 0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                to_return.append(element)\n",
    "            else:\n",
    "                if random.uniform(0, 1) > 0.5:\n",
    "                    i = random.randint(0, self.max_size - 1)\n",
    "                    to_return.append(self.data[i].clone())\n",
    "                    self.data[i] = element\n",
    "                else:\n",
    "                    to_return.append(element)\n",
    "        return torch.cat(to_return)\n",
    "    \n",
    "\n",
    "# Buffers of previously generated samples\n",
    "X_fake_buffer = ReplayBuffer()\n",
    "Y_fake_buffer = ReplayBuffer()\n",
    "    \n",
    "\n",
    "# Image transformations\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_height * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_height, img_width)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "# Training data loader\n",
    "dataloader_train = DataLoader(\n",
    "    ImageDataset(\"../data/%s\" % dataset_name, transforms_=transforms_, unaligned=True),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=10,\n",
    ")\n",
    "# Test data loader\n",
    "val_dataloader = DataLoader(\n",
    "    ImageDataset(\"../data/%s\" % dataset_name, transforms_=transforms_, unaligned=True, mode=\"test\"),\n",
    "    batch_size=5,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_features, in_features, 3),\n",
    "            nn.InstanceNorm2d(in_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class GeneratorResNet(nn.Module):\n",
    "    def __init__(self, input_shape, num_residual_blocks=9):\n",
    "        super(GeneratorResNet, self).__init__()\n",
    "\n",
    "        channels = input_shape[0]\n",
    "\n",
    "        # Initial convolution block\n",
    "        out_features = 64\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(channels),\n",
    "            nn.Conv2d(channels, out_features, 7),\n",
    "            nn.InstanceNorm2d(out_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        in_features = out_features\n",
    "\n",
    "        # Downsampling\n",
    "        for _ in range(2):\n",
    "            out_features *= 2\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Residual blocks\n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResidualBlock(out_features)]\n",
    "\n",
    "        # Upsampling\n",
    "        for _ in range(2):\n",
    "            out_features //= 2\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            in_features = out_features\n",
    "\n",
    "        # Output layer\n",
    "        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "##############################\n",
    "#        Discriminator\n",
    "##############################\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        channels, height, width = input_shape\n",
    "\n",
    "        # Calculate output shape of image discriminator (PatchGAN)\n",
    "        self.output_shape = (1, height // 2 ** 4, width // 2 ** 4)\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "    \n",
    "input_shape = (3, img_height, img_width)\n",
    "G = GeneratorResNet(input_shape, n_residual_blocks).to(device)\n",
    "F = GeneratorResNet(input_shape, n_residual_blocks).to(device)\n",
    "D_2 = Discriminator(input_shape).to(device)\n",
    "D_1 = Discriminator(input_shape).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_cycle = torch.nn.L1Loss()\n",
    "criterion_L1 = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    itertools.chain(G.parameters(), F.parameters()), lr=learning_rate, betas=(b1, b2)\n",
    ")\n",
    "optimizer_D_2 = torch.optim.Adam(D_2.parameters(), lr=learning_rate, betas=(b1, b2))\n",
    "optimizer_D_1 = torch.optim.Adam(D_1.parameters(), lr=learning_rate, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [11:55:48<6:25:23, 330.33s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss_G = 0.0\n",
    "    running_loss_D = 0.0\n",
    "    running_pixelwise_loss = 0.0\n",
    "    # Iterate over the training data\n",
    "    for idx_sample, batch in enumerate(dataloader_train):\n",
    "        # For example X_real (Real Satellite image)\n",
    "        # For example Y_real (Real Map image)\n",
    "        # X_real and Y_real are not alligned\n",
    "        X_real = batch[\"A\"].to(device)\n",
    "        Y_real = batch[\"B\"].to(device)\n",
    "        # Get Batch Size\n",
    "        batch_size = X_real.size()[0]\n",
    "        \n",
    "        # Adversarial ground truths (you can do soft-label here....)\n",
    "        # Remember that our discriminator outputs a grid of patches\n",
    "        # So valid/fake will be a spatial tensor\n",
    "        # On Python *tupple will expand the tupple\n",
    "        valid = torch.ones(batch_size, *patch).to(device)\n",
    "        fake = torch.zeros(batch_size, *patch).to(device)\n",
    "    \n",
    "        # Train Generators\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Identity loss (Part of cycle-consistent loss)\n",
    "        L1_A = criterion_L1(F(X_real), X_real)\n",
    "        L1_B = criterion_L1(G(Y_real), Y_real)\n",
    "        loss_identity = (L1_A + L1_B) / 2\n",
    "        \n",
    "        # GAN loss\n",
    "        Y_fake = G(X_real)\n",
    "        loss_GAN_AB = criterion_GAN(D_1(Y_fake), valid)\n",
    "        X_fake = F(Y_real)\n",
    "        loss_GAN_BA = criterion_GAN(D_2(X_fake), valid)\n",
    "        loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
    "        \n",
    "        # Cycle loss\n",
    "        X_recovered = F(Y_fake)\n",
    "        loss_cycle_A = criterion_cycle(X_recovered, X_real)\n",
    "        Y_recovered = G(X_fake)\n",
    "        loss_cycle_B = criterion_cycle(Y_recovered, Y_real)\n",
    "\n",
    "        loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G = loss_GAN + (lambda_cycle * loss_cycle) + (lambda_identity * loss_identity)\n",
    "\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Train Discriminator A\n",
    "        optimizer_D_2.zero_grad()\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_2(X_real), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        X_fake_ = X_fake_buffer.push_and_pop(X_fake)\n",
    "        loss_fake = criterion_GAN(D_2(X_fake_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_2 = (loss_real + loss_fake) / 2\n",
    "        loss_D_2.backward()\n",
    "        optimizer_D_2.step()\n",
    "        \n",
    "        # Train Discriminator B\n",
    "        optimizer_D_1.zero_grad()\n",
    "        # Real loss\n",
    "        loss_real = criterion_GAN(D_1(Y_real), valid)\n",
    "        # Fake loss (on batch of previously generated samples)\n",
    "        Y_fake_ = fake_B_buffer.push_and_pop(Y_fake)\n",
    "        loss_fake = criterion_GAN(D_1(Y_fake_.detach()), fake)\n",
    "        # Total loss\n",
    "        loss_D_1 = (loss_real + loss_fake) / 2\n",
    "        loss_D_1.backward()\n",
    "        optimizer_D_1.step()\n",
    "\n",
    "        # Calculate both discriminator losses\n",
    "        loss_D = (loss_D_2 + loss_D_1) / 2\n",
    "        \n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss_G += loss_G.item() * batch_size\n",
    "        # Update statistics\n",
    "        running_loss_D += loss_D.item() * batch_size\n",
    "    \n",
    "    # Epoch ends\n",
    "    epoch_loss_generator = running_loss_G / len(dataloader_train.dataset)\n",
    "    epoch_loss_discriminator = running_loss_D / len(dataloader_train.dataset)\n",
    "    \n",
    "    # Send results to tensorboard\n",
    "    writer.add_scalar('train/loss_generator', epoch_loss_generator, epoch)\n",
    "    writer.add_scalar('train/loss_discriminator', epoch_loss_discriminator, epoch)\n",
    "    \n",
    "    # Send images to tensorboard\n",
    "    writer.add_images('train/X_real', X_real, epoch)\n",
    "    writer.add_images('train/Y_real', Y_real, epoch)\n",
    "    writer.add_images('train/F', X_fake, epoch)\n",
    "    writer.add_images('train/G', Y_fake, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
