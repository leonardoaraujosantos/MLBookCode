{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Adversarial AutoEncoder\n",
    "Implementation vanilla (no CNN) Unconditioned Adversarial AutoEncoder. This architecture forces the latent space of an autoencoder to follow a particular distribution, in a way more efficient than Variational AutoEncoders.\n",
    "\n",
    "In other words it makes a simple autoencoder become a generative model.\n",
    "\n",
    "#### What you can do\n",
    "* Semi-supervised classification\n",
    "* Generative Modeling (Unconditioned and Conditioned)\n",
    "* Dimensionaliry Reduction\n",
    "* Clustering\n",
    "\n",
    "#### Losses\n",
    "* Reconstruction Loss\n",
    "* Discriminator Loss\n",
    "* Generator (encoder) Loss\n",
    "\n",
    "The adversarial training criterion forces the autoencoder latent follow any particular distribution.\n",
    "\n",
    "#### References\n",
    "* [Paper](https://arxiv.org/pdf/1511.05644.pdf)\n",
    "* https://github.com/neale/Adversarial-Autoencoder\n",
    "* https://github.com/bfarzin/pytorch_aae\n",
    "* https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Pytorch version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import mnist_data_pytorch as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -rf ./runs\n",
    "writer = SummaryWriter('./runs/train')\n",
    "\n",
    "# Metaparameters\n",
    "num_epochs = 30\n",
    "latent_size = 100\n",
    "gen_lr = 0.0001\n",
    "reg_lr = 0.00005\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Encoder/Decoder/Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.latent = nn.Linear(1000, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        z = self.latent(x)\n",
    "        return z\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.lin3 = nn.Linear(1000, X_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):  \n",
    "    def __init__(self, z_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, 500)\n",
    "        self.lin2 = nn.Linear(500, 500)\n",
    "        self.lin3 = nn.Linear(500, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        return torch.sigmoid(self.lin3(x)) \n",
    "\n",
    "\n",
    "# Initialize Networks\n",
    "encoder = Encoder(784, latent_size).to(device)\n",
    "decoder = Decoder(784, latent_size).to(device)\n",
    "discriminator = Discriminator(latent_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_encoder = torch.optim.Adam(encoder.parameters(), lr=gen_lr)\n",
    "optim_decoder = torch.optim.Adam(decoder.parameters(), lr=gen_lr)\n",
    "#regularizing optimizers\n",
    "optim_encoder_generator = torch.optim.Adam(encoder.parameters(), lr=reg_lr)\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:16<00:00,  2.56s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss_discriminator = 0.0\n",
    "    running_loss_generator = 0.0\n",
    "    running_loss_reconstruction = 0.0\n",
    "    # Iterate over the data\n",
    "    for idx_sample, (inputs, _) in enumerate(data.dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = torch.flatten(inputs, start_dim=1, end_dim=-1)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optim_encoder.zero_grad()\n",
    "        optim_decoder.zero_grad()\n",
    "        optim_discriminator.zero_grad()\n",
    "        \n",
    "        z_sample = encoder(inputs)\n",
    "        inputs_reconstruct = decoder(z_sample) #decode to X reconstruction\n",
    "        reconstruct_loss = F.binary_cross_entropy(inputs_reconstruct + EPS, inputs + EPS)\n",
    "        \n",
    "        # Backprop from reconstruction loss\n",
    "        reconstruct_loss.backward()\n",
    "        # Optimizer Encoder/Decoder\n",
    "        optim_encoder.step()\n",
    "        optim_decoder.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss_reconstruction += reconstruct_loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Discriminator\n",
    "        ## true prior is random normal (randn)\n",
    "        ## this is constraining the Z-projection to be normal!\n",
    "        encoder.eval()\n",
    "        batch_size = inputs.size()[0]\n",
    "        z_real_gauss = (torch.randn(batch_size, latent_size) * 1.).to(device)\n",
    "        D_real_gauss = discriminator(z_real_gauss)\n",
    "\n",
    "        # Fake images come from encoder(generator)\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + EPS) + torch.log(1 - D_fake_gauss + EPS))\n",
    "\n",
    "        D_loss.backward()\n",
    "        optim_discriminator.step()\n",
    "        # Update statistics\n",
    "        running_loss_discriminator += D_loss.item() * inputs.size(0)\n",
    "\n",
    "        # Generator\n",
    "        encoder.train()\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + EPS))\n",
    "\n",
    "        G_loss.backward()\n",
    "        optim_encoder_generator.step()   \n",
    "        # Update statistics\n",
    "        running_loss_generator += G_loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Epoch ends\n",
    "    epoch_loss_generator = running_loss_generator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_discriminator = running_loss_discriminator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_reconstruction = running_loss_reconstruction / len(data.dataloaders['train'].dataset)\n",
    "    \n",
    "    # Send results to tensorboard\n",
    "    writer.add_scalar('train/loss_generator', epoch_loss_generator, epoch)\n",
    "    writer.add_scalar('train/loss_discriminator', epoch_loss_discriminator, epoch)\n",
    "    writer.add_scalar('train/reconstruction', epoch_loss_reconstruction, epoch)\n",
    "    \n",
    "    # Send images to tensorboard\n",
    "    writer.add_images('train/decoder_images', inputs_reconstruct.view(inputs.size(0),1,28,28), epoch)\n",
    "    \n",
    "    # Send latent to tensorboard\n",
    "    writer.add_histogram('train/latent', z_sample, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples (Unconditioned)\n",
    "Observe that the generated samples are somehow a mix of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_idx=0):\n",
    "    decoder.eval()\n",
    "    z_real_gauss = (torch.randn(1, latent_size) * 5.).to(device)\n",
    "    with torch.no_grad(): \n",
    "        generated_sample = decoder(z_real_gauss)\n",
    "\n",
    "    plt.imshow(generated_sample.view(28,28).cpu().numpy())\n",
    "    plt.title('Generated sample')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e54780a6fd9463c989696399c23affb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='num_idx'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(generate_sample, num_idx=widgets.IntSlider(min=0, max=100, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAS2klEQVR4nO3de5ScdX3H8fdnL0lIQiDLJYaLIIgXoBJ1JQhWsagF9BzwWtIjjVYbakWlpady8FSjPbbUCxeP1ygo3kCqItRDKZijUlpJWShXo3IxkpCQJSSYC7ns7nz7x/MkDsvObzY7szMTfp/XOXN25vk+z853njOfeWaeqyICM3v262p3A2bWGg67WSYcdrNMOOxmmXDYzTLhsJtlwmG33SbpXZJubXcfKZJWSHpdu/voJA57k0g6S9IySVskDZb3/0aS2t3baJJ+Jum97e7DWsthbwJJ5wOXAZ8GngPMAf4aOAmY0uJeelr5fLYHiQjfGrgB+wBbgLfWGW8q8BngEWAt8GVgr7J2MrAKOB8YBNYA797NaT8MPAZ8C5gN/Bh4HNhQ3j+kHP+TwAiwDdgMfL4c/iLgZmA98GvgHVXPvx9wPbAR+F/gn4Bba7zOacC3gSeAJ4HbgTll7d3AcmAT8DBwTtV0O1/HP1TNgzOB04HflH1dWDX+YuD7wPfK/3cncFxVfQXwuvJ+F3AB8FDZ1zVAX7vfOy1/r7a7gT39BpwKDAM9dca7tAxMH7A38O/Av5S1k8v/8Qmgt3yDPwXM3o1p/7X8UNirDOdbgenl+P8G/Kiql58B7616PANYWYaxB3gZsA44pqxfXQZkBnAs8Ggi7OeU/U0HuoGXA7PK2huBIwEBrylf48tGvY6PlvPgryg+rL5bvoZjKD6gjijHXwwMAW8rx/974LdAb1mvDvt5wG3AIeU8+gpwVbvfOy1/r7a7gT39BrwTeGzUsP+hWKptBV5dvrm3AEdWjfNK4Lfl/ZPLcXuq6oPACeOcdgcwLdHjPGBD1ePRYf8z4L9GTfMV4GNlYIeAF1XV/jkR9r8sX/9LxjHvfgR8aNQ86C4f7w0EML9q/DuAM8v7i4HbqmpdFN8G/rh8XB325cApVePOLV9T8gP62Xbz77vGPQHsL6knIoYBIuJEAEmrKN6EB1As6e6oWl8niiDt+j87py89Bcwc57SPR8S2XUVpOnAJxbeO2eXgvSV1R8TIGK/hMGC+pCerhvVQ/CQ4oLy/sqr2u7FnBZTTHApcLWlfiq/0H4mIIUmnUXyAvIBivkwH7h01D3b2t7X8u7aqvpVinuy0q6eIqJTz+6Aar+9aSZWqYSMU61YeTbyWZxWvoGvcL4DtwBmJcdZRvFGPiYh9y9s+ETEzMc3uTDv60MXzgRdSLBVnUXy7gOJDYqzxVwI/r/r/+0bEzIh4H8VX6WGKAO/03FrNRsRQRHw8Io4GTgTeBPyFpKnADyjWPcyJiH2BG6p6mohdPUnqoviavnqM8VYCp416fdMiIpugg8PesIh4Evg48EVJb5M0U1KXpHkUv3GJiArwVeASSQcCSDpY0p+O4/9PZNq9KT4gnpTUR7E0rbYWOKLq8Y+BF0g6W1JveXuFpBeXS9ofAoslTZd0NLCw1hNLeq2kP5LUTbFCb4hiKTqF4vfy48BwuZR/Q73XX8fLJb2l3AJxHsWH7m1jjPdl4JOSDit7PEBS6sP5Wclhb4KI+BTwd/xhTfJait+8H6b4/Up5/0HgNkkbgZ9QLH3HY3envZRiRd06ijf/jaPqlwFvk7RB0uciYhNF8M6iWDI+xh9W+AGcS/H1+THgG8DXE8/9HIq15Bspfiv/HPh2+RwfpFjRtwH4c4qVjo24jmJ9wwbgbOAtETE0xniXlc91k6RNFPNkfoPPvcdRucLCbI8iaTHw/Ih4Z7t72VN4yW6WCYfdLBP+Gm+WCS/ZzTLR0p1qpmhqTCu2RpnZJNjGFnbE9jH3XWgo7JJOpdis0Q18LSIuSo0/jRnM1ymNPKWZJSyLpTVrE/4aX+408QXgNOBoYEG5w4WZdaBGfrMfDzwYEQ9HxA6KI6Oy2yvJbE/RSNgP5ukHR6wqhz2NpEWSBiQNDLG9gaczs0Y0EvaxVgI8YzteRCyJiP6I6O/dtfelmbVaI2FfxdOPhKp1xJGZdYBGwn47cJSk50maQnEQRaMHNpjZJJnwpreIGJZ0LvCfFJveroiI+5vWmZk1VUPb2SPiBooTEJhZh/PusmaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhx2s0w47GaZcNjNMuGwm2XCYTfLhMNulomGruJqNqmkZLl7n1np6ffvq1mqrFiZnDSGh9P/ew/UUNglrQA2ASPAcET0N6MpM2u+ZizZXxsR65rwf8xsEvk3u1kmGg17ADdJukPSorFGkLRI0oCkgSG2N/h0ZjZRjX6NPykiVks6ELhZ0q8i4pbqESJiCbAEYJb6osHnM7MJamjJHhGry7+DwLXA8c1oysyab8JhlzRD0t477wNvAO5rVmNm1lyNfI2fA1yrYltoD/DdiLixKV1Zx9DUqcl61/MPT9Z/f/S+tWsLNienvey4q5P1V03blqyPRO1fjddvmZOcdskH3pqs9940kKx3ogmHPSIeBo5rYi9mNom86c0sEw67WSYcdrNMOOxmmXDYzTLhQ1yf5SqveWmyfs23v5Csz+qaVucZfpGsbo/ah4oOMZKcdqbSm/261ZuskzhC9u0zn0hO+sXz0/Xen3Snn7uSfm3t4CW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJb2ffE9Q5pfLg+15Zs/bfF16anHZ61/QJtbTTI8Ppw1S//MSJNWtHThtMTvuWmQ8n6/X2AahQ+xDXzZX0KdK2DaejsVdUkvVO5CW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJb2ffA6gnfdz21I21t/ke+6MPJKc9YFn6836/Gx9M1isbNybrjNQ+rvv/jv2T5KTXXpo+Fv/EvvR2+FvWPb9m7dEbD0tO+9xvPpSsDydOU92pvGQ3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTKhaOH2wlnqi/k6pWXPZx2gq/b51btn75Oc9Ik3vjBZ3/c3W5L17l+uqFmL4drnsweobN2arNOh29mXxVI2xvoxT4BQd8ku6QpJg5LuqxrWJ+lmSQ+Uf2c3s2Eza77xfI3/BnDqqGEXAEsj4ihgafnYzDpY3bBHxC3A+lGDzwCuLO9fCZzZ5L7MrMkmuoJuTkSsASj/HlhrREmLJA1IGhgifd4vM5s8k742PiKWRER/RPT3kr5Qn5lNnomGfa2kuQDl3/RpQs2s7SYa9uuBheX9hcB1zWnHzCZL3ePZJV0FnAzsL2kV8DHgIuAaSe8BHgHePplNWgerc077mH9szdrGg9Lnfd/vP9LH0o+sW5eud+i28HapG/aIWFCj5L1jzPYg3l3WLBMOu1kmHHazTDjsZplw2M0y4VNJt0KdzVOdergkQNf09CWdN3z/oGT960d/qWbt4rWvT0678sfpw1A7eb51Ii/ZzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMeDv7eCW2lXdNTZ+Bp+HTdVcmPr2605/nm950XLJ+8ac/n6y/fErtU0UXah/G+rOHjkpOecSOu+v8b9sdXrKbZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZpnwdvZxUnft7cmVeS9ITzuS3k7+1EF7Jeu9m0aS9W3799asPXlk+vP8qnMuTtaP6Z2SrNezdGvtfRCO+tvVyWl9Kujm8pLdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEt7M3QfcDq5L1rf1HpKffVknWoyd93vl97t9Qs/bUgfslp/3Eyjcl6wft9ftk/afXvCJZP+Rzd9asVbYNJqe15qq7ZJd0haRBSfdVDVss6VFJd5W30ye3TTNr1Hi+xn8DOHWM4ZdExLzydkNz2zKzZqsb9oi4BVjfgl7MbBI1soLuXEn3lF/zZ9caSdIiSQOSBobY3sDTmVkjJhr2LwFHAvOANcBna40YEUsioj8i+ntJn5jRzCbPhMIeEWsjYiQiKsBXgeOb25aZNduEwi5pbtXDNwP31RrXzDqD6p3TXNJVwMnA/sBa4GPl43lAACuAcyJiTb0nm6W+mK9TGmq4XZQ4N7zqXH9d09I/XypbtibrMZI+np1KnbplY1ksZWOsH/MNWXenmohYMMbgyxvuysxayrvLmmXCYTfLhMNulgmH3SwTDrtZJnyI6zjF0HDtYuI00wCVJ9OHiZq1gpfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmvJ19vKL26Z5jOH0qaLNO4CW7WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpYJh90sEw67WSYcdrNMOOxmmXDYzTLhsJtlwmE3y4TDbpaJusezSzoU+CbwHKACLImIyyT1Ad8DDqe4bPM7ImLD5LXaudTTm6zH0I4WddJ8PYcekqz/8h/nJuvTVtWeN4d/9u7ktJUtW5J12z3jWbIPA+dHxIuBE4D3SzoauABYGhFHAUvLx2bWoeqGPSLWRMSd5f1NwHLgYOAM4MpytCuBMyerSTNr3G79Zpd0OPBSYBkwJyLWQPGBABzY7ObMrHnGHXZJM4EfAOdFxMbdmG6RpAFJA0Nsn0iPZtYE4wq7pF6KoH8nIn5YDl4raW5ZnwsMjjVtRCyJiP6I6O9lajN6NrMJqBt2SQIuB5ZHxMVVpeuBheX9hcB1zW/PzJplPKeSPgk4G7hX0l3lsAuBi4BrJL0HeAR4++S02Pm6Zs5I1iub0qeajuHE5aAn2wkvSZa/9/0lyXqv0perPu7yD9asVZ56KjmtNVfdsEfErYBqlE9pbjtmNlm8B51ZJhx2s0w47GaZcNjNMuGwm2XCYTfLhC/Z3Azd6c/MkROOTdZ77n4oWa9s3rzbLe3Uvc+sZP11X7s1Wd9USe8DsOBXZyXrh330F8m6tY6X7GaZcNjNMuGwm2XCYTfLhMNulgmH3SwTDrtZJhQRLXuyWeqL+Xr2HRWrnvTuCvXqXXMOSD/BjqHdbWmXmJ3ezh496c97rVidrI9sHPcZyqwFlsVSNsb6MQ9J95LdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEj2dvgnrnfa9Xr/xuZTPbebo1j03e/7Y9ipfsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1km6oZd0qGSfippuaT7JX2oHL5Y0qOS7ipvp09+u2Y2UePZqWYYOD8i7pS0N3CHpJvL2iUR8ZnJa8/MmqVu2CNiDbCmvL9J0nLg4MluzMyaa7d+s0s6HHgpsKwcdK6keyRdIWl2jWkWSRqQNDDE9oaaNbOJG3fYJc0EfgCcFxEbgS8BRwLzKJb8nx1ruohYEhH9EdHfy9QmtGxmEzGusEvqpQj6dyLihwARsTYiRiKiAnwVOH7y2jSzRo1nbbyAy4HlEXFx1fC5VaO9Gbiv+e2ZWbOMZ238ScDZwL2S7iqHXQgskDQPCGAFcM6kdGhmTTGetfG3AmOdh/qG5rdjZpPFe9CZZcJhN8uEw26WCYfdLBMOu1kmHHazTDjsZplw2M0y4bCbZcJhN8uEw26WCYfdLBMOu1kmHHazTCgiWvdk0uPA76oG7Q+sa1kDu6dTe+vUvsC9TVQzezssIg4Yq9DSsD/jyaWBiOhvWwMJndpbp/YF7m2iWtWbv8abZcJhN8tEu8O+pM3Pn9KpvXVqX+DeJqolvbX1N7uZtU67l+xm1iIOu1km2hJ2SadK+rWkByVd0I4eapG0QtK95WWoB9rcyxWSBiXdVzWsT9LNkh4o/455jb029dYRl/FOXGa8rfOu3Zc/b/lvdkndwG+A1wOrgNuBBRHxy5Y2UoOkFUB/RLR9BwxJrwY2A9+MiGPLYZ8C1kfEReUH5eyI+HCH9LYY2Nzuy3iXVyuaW32ZceBM4F20cd4l+noHLZhv7ViyHw88GBEPR8QO4GrgjDb00fEi4hZg/ajBZwBXlvevpHiztFyN3jpCRKyJiDvL+5uAnZcZb+u8S/TVEu0I+8HAyqrHq+is670HcJOkOyQtanczY5gTEWugePMAB7a5n9HqXsa7lUZdZrxj5t1ELn/eqHaEfaxLSXXS9r+TIuJlwGnA+8uvqzY+47qMd6uMcZnxjjDRy583qh1hXwUcWvX4EGB1G/oYU0SsLv8OAtfSeZeiXrvzCrrl38E297NLJ13Ge6zLjNMB866dlz9vR9hvB46S9DxJU4CzgOvb0MczSJpRrjhB0gzgDXTepaivBxaW9xcC17Wxl6fplMt417rMOG2ed22//HlEtPwGnE6xRv4h4CPt6KFGX0cAd5e3+9vdG3AVxde6IYpvRO8B9gOWAg+Uf/s6qLdvAfcC91AEa26bensVxU/De4C7ytvp7Z53ib5aMt+8u6xZJrwHnVkmHHazTDjsZplw2M0y4bCbZcJhN8uEw26Wif8Hn1HbyVEMEuIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
