{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Adversarial AutoEncoder\n",
    "Implementation vanilla (no CNN) Unconditioned Adversarial AutoEncoder. This architecture forces the latent space of an autoencoder to follow a particular distribution, in a way more efficient than Variational AutoEncoders.\n",
    "\n",
    "In other words it makes a simple autoencoder become a generative model.\n",
    "\n",
    "#### What you can do\n",
    "* Semi-supervised classification\n",
    "* Generative Modeling (Unconditioned and Conditioned)\n",
    "* Dimensionaliry Reduction\n",
    "* Clustering\n",
    "\n",
    "#### Losses\n",
    "* Reconstruction Loss\n",
    "* Discriminator Loss\n",
    "* Generator (encoder) Loss\n",
    "\n",
    "The adversarial training criterion forces the autoencoder latent follow any particular distribution.\n",
    "\n",
    "#### References\n",
    "* [Paper](https://arxiv.org/pdf/1511.05644.pdf)\n",
    "* https://github.com/neale/Adversarial-Autoencoder\n",
    "* https://github.com/bfarzin/pytorch_aae\n",
    "* https://blog.paperspace.com/adversarial-autoencoders-with-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n",
      "Pytorch version: 1.2.0\n"
     ]
    }
   ],
   "source": [
    "import mnist_data_pytorch as data\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "print('Pytorch version:', torch.__version__)\n",
    "# Tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "!rm -rf ./runs\n",
    "writer = SummaryWriter('./runs/train')\n",
    "\n",
    "# Metaparameters\n",
    "num_epochs = 30\n",
    "latent_size = 100\n",
    "gen_lr = 0.0001\n",
    "reg_lr = 0.00005\n",
    "EPS = 1e-15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Encoder/Decoder/Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(X_dim, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.latent = nn.Linear(1000, z_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        z = self.latent(x)\n",
    "        return z\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):  \n",
    "    def __init__(self, X_dim, z_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, 1000)\n",
    "        self.lin2 = nn.Linear(1000, 1000)\n",
    "        self.lin3 = nn.Linear(1000, X_dim)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.25, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.25, training=self.training)\n",
    "        x = self.lin3(x)\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):  \n",
    "    def __init__(self, z_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.lin1 = nn.Linear(z_dim, 500)\n",
    "        self.lin2 = nn.Linear(500, 500)\n",
    "        self.lin3 = nn.Linear(500, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.dropout(self.lin1(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(self.lin2(x), p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        return torch.sigmoid(self.lin3(x)) \n",
    "\n",
    "\n",
    "# Initialize Networks\n",
    "encoder = Encoder(784, latent_size).to(device)\n",
    "decoder = Decoder(784, latent_size).to(device)\n",
    "discriminator = Discriminator(latent_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_encoder = torch.optim.Adam(encoder.parameters(), lr=gen_lr)\n",
    "optim_decoder = torch.optim.Adam(decoder.parameters(), lr=gen_lr)\n",
    "#regularizing optimizers\n",
    "optim_encoder_generator = torch.optim.Adam(encoder.parameters(), lr=reg_lr)\n",
    "optim_discriminator = torch.optim.Adam(discriminator.parameters(), lr=reg_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:25<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss_discriminator = 0.0\n",
    "    running_loss_generator = 0.0\n",
    "    running_loss_reconstruction = 0.0\n",
    "    # Iterate over the data\n",
    "    for idx_sample, (inputs, _) in enumerate(data.dataloaders['train']):\n",
    "        inputs = inputs.to(device)\n",
    "        inputs = torch.flatten(inputs, start_dim=1, end_dim=-1)\n",
    "        # Normalize inputs\n",
    "        #inputs = inputs * 0.3081 + 0.1307\n",
    "        \n",
    "        # Zero gradients\n",
    "        optim_encoder.zero_grad()\n",
    "        optim_decoder.zero_grad()\n",
    "        optim_discriminator.zero_grad()\n",
    "        \n",
    "        z_sample = encoder(inputs)\n",
    "        \n",
    "        # Reconstruct X\n",
    "        inputs_reconstruct = decoder(z_sample) \n",
    "        reconstruct_loss = F.binary_cross_entropy(inputs_reconstruct + EPS, inputs + EPS)\n",
    "        \n",
    "        # Backprop from reconstruction loss\n",
    "        reconstruct_loss.backward()\n",
    "        # Optimizer Encoder/Decoder\n",
    "        optim_encoder.step()\n",
    "        optim_decoder.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss_reconstruction += reconstruct_loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Discriminator\n",
    "        ## true prior is random normal (randn)\n",
    "        ## this is constraining the Z-projection to be normal!\n",
    "        encoder.eval()\n",
    "        batch_size = inputs.size()[0]\n",
    "        z_real_distribution = (torch.randn(batch_size, latent_size) * 5.).to(device)\n",
    "        D_real_gauss = discriminator(z_real_distribution)\n",
    "\n",
    "        # Fake images come from encoder(generator)\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        D_loss = -torch.mean(torch.log(D_real_gauss + EPS) + torch.log(1 - D_fake_gauss + EPS))\n",
    "\n",
    "        D_loss.backward()\n",
    "        optim_discriminator.step()\n",
    "        # Update statistics\n",
    "        running_loss_discriminator += D_loss.item() * inputs.size(0)\n",
    "\n",
    "        # Generator\n",
    "        encoder.train()\n",
    "        z_fake_gauss = encoder(inputs)\n",
    "        D_fake_gauss = discriminator(z_fake_gauss)\n",
    "\n",
    "        G_loss = -torch.mean(torch.log(D_fake_gauss + EPS))\n",
    "\n",
    "        G_loss.backward()\n",
    "        optim_encoder_generator.step()   \n",
    "        # Update statistics\n",
    "        running_loss_generator += G_loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Epoch ends\n",
    "    epoch_loss_generator = running_loss_generator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_discriminator = running_loss_discriminator / len(data.dataloaders['train'].dataset)\n",
    "    epoch_loss_reconstruction = running_loss_reconstruction / len(data.dataloaders['train'].dataset)\n",
    "    \n",
    "    # Send results to tensorboard\n",
    "    writer.add_scalar('train/loss_generator', epoch_loss_generator, epoch)\n",
    "    writer.add_scalar('train/loss_discriminator', epoch_loss_discriminator, epoch)\n",
    "    writer.add_scalar('train/reconstruction', epoch_loss_reconstruction, epoch)\n",
    "    \n",
    "    # Send images to tensorboard\n",
    "    writer.add_images('train/decoder_images', inputs_reconstruct.view(inputs.size(0),1,28,28), epoch)\n",
    "    writer.add_images('train/input_images', inputs.view(inputs.size(0),1,28,28), epoch)\n",
    "    \n",
    "    # Send latent to tensorboard\n",
    "    writer.add_histogram('train/latent', z_sample, epoch)\n",
    "    writer.add_histogram('train/distribution', z_real_distribution, epoch)\n",
    "    writer.add_histogram('train/input_images_h', inputs, epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Samples (Unconditioned)\n",
    "Observe that the generated samples are somehow a mix of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(num_idx=0):\n",
    "    decoder.eval()\n",
    "    z_real_gauss = (torch.randn(1, latent_size) * 5.).to(device)\n",
    "    with torch.no_grad(): \n",
    "        generated_sample = decoder(z_real_gauss)\n",
    "\n",
    "    plt.imshow(generated_sample.view(28,28).cpu().numpy())\n",
    "    plt.title('Generated sample')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b7d39e54be444585fd5765be8bd581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='num_idx'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "interact(generate_sample, num_idx=widgets.IntSlider(min=0, max=100, step=1, value=0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
